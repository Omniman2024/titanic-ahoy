{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-29T09:58:50.316121Z","iopub.execute_input":"2025-12-29T09:58:50.316437Z","iopub.status.idle":"2025-12-29T09:58:50.326560Z","shell.execute_reply.started":"2025-12-29T09:58:50.316411Z","shell.execute_reply":"2025-12-29T09:58:50.325784Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Loading the data\ntrain_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T09:58:52.025280Z","iopub.execute_input":"2025-12-29T09:58:52.025618Z","iopub.status.idle":"2025-12-29T09:58:52.040470Z","shell.execute_reply.started":"2025-12-29T09:58:52.025590Z","shell.execute_reply":"2025-12-29T09:58:52.039370Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Feature Engineering\n\n# Creating a new column containing Family size of the passenger travelling\ntrain_data['Family_Size'] = train_data['SibSp'] + train_data['Parch'] + 1\ntest_data['Family_Size'] = test_data['SibSp'] + test_data['Parch'] + 1\n\n# Creating a new column called as Title for studying the relationship between age and survival\ntrain_data['Title'] = train_data['Name'].str.extract(r' ([A-Za-z]+)\\.', expand=False)\ntrain_data['Title'] = train_data['Title'].replace(['Lady', 'Countess','Capt', 'Col', \n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain_data['Title'] = train_data['Title'].replace('Mme', 'Mrs')\ntrain_data['Title'] = train_data['Title'].replace(['Mlle', 'Ms'], 'Miss')\ntest_data['Title'] = test_data['Name'].str.extract(r' ([A-Za-z]+)\\.', expand=False)\ntest_data['Title'] = test_data['Title'].replace(['Lady', 'Countess','Capt', 'Col', \n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest_data['Title'] = test_data['Title'].replace('Mme', 'Mrs')\ntest_data['Title'] = test_data['Title'].replace(['Mlle', 'Ms'], 'Miss')\n# Filling the gaps in the age column with the median of the title group they belong to\ntrain_data['Age'] = train_data['Age'].fillna(train_data.groupby('Title')['Age'].transform('median'))\ntest_data['Age'] = test_data['Age'].fillna(test_data.groupby('Title')['Age'].transform('median'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:06:47.922662Z","iopub.execute_input":"2025-12-29T10:06:47.923018Z","iopub.status.idle":"2025-12-29T10:06:47.942731Z","shell.execute_reply.started":"2025-12-29T10:06:47.922989Z","shell.execute_reply":"2025-12-29T10:06:47.941828Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint, uniform\n\ny = train_data[\"Survived\"]\nfeatures = [\"Pclass\", \"Sex\", \"Family_Size\", \"Title\", \"Age\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = XGBClassifier( \n    verbosity=0,\n    random_state = 55)\n\n\nparam_dist = {\n    'n_estimators': randint(100, 1000),\n    'learning_rate': uniform(0.01, 0.2), \n    'max_depth': randint(1, 10),\n    'subsample': uniform(0.5, 0.5) \n}\n\nsearch = RandomizedSearchCV(\n    estimator=model,\n    param_distributions=param_dist,\n    n_iter=50,       \n    cv=3,            \n    verbose=0,\n    n_jobs=-1, \n    random_state = 55\n)\n\nsearch.fit(X, y)\npredictions = search.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T10:06:50.735233Z","iopub.execute_input":"2025-12-29T10:06:50.735583Z","iopub.status.idle":"2025-12-29T10:07:03.504165Z","shell.execute_reply.started":"2025-12-29T10:06:50.735553Z","shell.execute_reply":"2025-12-29T10:07:03.503471Z"}},"outputs":[{"name":"stdout","text":"Your submission was successfully saved!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}